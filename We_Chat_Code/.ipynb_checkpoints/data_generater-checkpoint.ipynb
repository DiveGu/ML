{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储数据的根目录\n",
    "ROOT_PATH = \"/testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data\"\n",
    "# 比赛数据集路径\n",
    "DATASET_PATH = os.path.join(ROOT_PATH, \"wechat_algo_data1\")\n",
    "# 训练集\n",
    "USER_ACTION = os.path.join(DATASET_PATH, \"user_action.csv\")\n",
    "FEED_INFO = os.path.join(DATASET_PATH, \"feed_info.csv\")\n",
    "FEED_EMBEDDINGS = os.path.join(DATASET_PATH, \"feed_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "TEST_FILE = os.path.join(DATASET_PATH, \"test_a.csv\")\n",
    "END_DAY = 15\n",
    "SEED = 1997\n",
    "\n",
    "# 初赛待预测行为列表\n",
    "ACTION_LIST = [\"read_comment\", \"like\", \"click_avatar\",  \"forward\"]\n",
    "# ACTION_LIST = [\"read_comment\", \"like\"]\n",
    "# ACTION_LIST = [\"click_avatar\",  \"forward\"]\n",
    "# 复赛待预测行为列表\n",
    "# ACTION_LIST = [\"read_comment\", \"like\", \"click_avatar\",  \"forward\", \"comment\", \"follow\", \"favorite\"]\n",
    "# 用于构造特征的字段列表\n",
    "FEA_COLUMN_LIST = [\"read_comment\", \"like\", \"click_avatar\",  \"forward\", \"comment\", \"follow\", \"favorite\"]\n",
    "# 负样本下采样比例(负样本:正样本)\n",
    "ACTION_SAMPLE_RATE = {\"read_comment\": 15, \"like\": 15, \"click_avatar\": 10, \"forward\": 10, \"comment\": 10, \"follow\": 10, \"favorite\": 10}\n",
    "# 各个阶段数据集的设置的最后一天\n",
    "STAGE_END_DAY = {\"online_train\": 14, \"offline_train\": 12, \"evaluate\": 13, \"submit\": 15}\n",
    "# 各个行为构造训练数据的天数\n",
    "ACTION_DAY_NUM = {\"read_comment\": 14, \"like\": 14, \"click_avatar\": 14, \"forward\": 14, \"comment\": 14, \"follow\": 14, \"favorite\": 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样History数据\n",
    "def generate_sample(day=14,stage=\"offline_train\"):\n",
    "    \"\"\"\n",
    "    对负样本进行下采样，生成各个阶段所需样本\n",
    "    \"\"\"\n",
    "    df_arr=[]\n",
    "    sample_path=USER_ACTION\n",
    "    df = pd.read_csv(sample_path)\n",
    "    \n",
    "    # 线下/线上训练\n",
    "    # 去除重复行为，同行为取按时间最近的样本\n",
    "    for action in ACTION_LIST:\n",
    "        df = df.drop_duplicates(subset=['userid', 'feedid', action], keep='last')\n",
    "    # 负样本下采样\n",
    "    for action in ACTION_LIST:\n",
    "        # 为每个行为选近期 ACTION_DAY_NUM 天的历史行为作为history\n",
    "        # [8,9,10,11,12,13,14]\n",
    "        action_df = df[(df[\"date_\"] <= day) & (df[\"date_\"] >= day - ACTION_DAY_NUM[action] + 1)]\n",
    "        df_neg = action_df[action_df[action] == 0]\n",
    "        \n",
    "#         # 对于较难的 read_comment like 重新设计负采样\n",
    "#         if(action in ['read_comment','like',\"click_avatar\",  \"forward\"]):\n",
    "#             design_action=['read_comment','like','follow','favorite','forward','comment',\"click_avatar\",]\n",
    "#             design_action.remove(action)\n",
    "#             df_neg_design=df_neg[(df_neg[design_action[0]]==1) | (df_neg[design_action[1]]==1) | (df_neg[design_action[2]]==1) |\n",
    "#                                  (df_neg[design_action[3]]==1) | (df_neg[design_action[4]]==1)| (df_neg[design_action[5]]==1)]\n",
    "#             df_neg=df_neg[~df_neg.index.isin(df_neg_design.index)]\n",
    "            \n",
    "        all_pos_num=len(action_df[action_df[action] == 1])\n",
    "        all_neg_num=len(action_df)-all_pos_num\n",
    "        \n",
    "        sample_neg_num=min(len(df_neg),all_pos_num*ACTION_SAMPLE_RATE[action])\n",
    "        df_neg=df_neg.sample(n=sample_neg_num, random_state=SEED, replace=False)\n",
    "        print('-----------{}-------------'.format(action))\n",
    "        print('pos num:{};neg num:{}'.format(all_pos_num,sample_neg_num))\n",
    "        # 为每个aciton进行负采样\n",
    "#         df_neg = df_neg[df_neg['date_']<day].sample(n=sample_neg_num, random_state=SEED, replace=False)\n",
    "        # 按照停留时间进行采样\n",
    "#         df_neg=df_neg.sort_values(by='stay',ascending=False)\n",
    "#         df_neg = df_neg[:sample_neg_num]\n",
    "        # 正样本 负样本concat\n",
    "    \n",
    "        df_all = pd.concat([df_neg,action_df[action_df[action] == 1]])\n",
    "#         df_all = pd.concat([df_neg, action_df[action_df[action] == 1],action_df[(action_df[action] == 0)&(action_df['date_'] == day)]])\n",
    "            \n",
    "        col = [\"userid\", \"feedid\", \"date_\", \"device\"] + ACTION_LIST\n",
    "        file_path='{}/generater_data/{}_{}_sample.csv'.format(ROOT_PATH,action,day)\n",
    "        print('Save to: {}'.format(file_path))\n",
    "        df_all[col].to_csv(file_path, index=False)\n",
    "        print(df_all[col].shape)\n",
    "        df_arr.append(df_all[col])\n",
    "    return df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过读取采样History数据 获取所有df\n",
    "def get_generate_sample(day):\n",
    "    df_arr=[]\n",
    "    for action in ACTION_LIST:\n",
    "        file_path='{}/generater_data/{}_{}_sample.csv'.format(ROOT_PATH,action,day)\n",
    "        tmp=pd.read_csv(file_path)\n",
    "        print('--------{}---------'.format(action))\n",
    "        print(tmp.shape)\n",
    "        pos_num,neg_num=len(tmp[tmp[action]==1]),len(tmp[tmp[action]==0])\n",
    "        print('pos num:{};neg num:{}'.format(pos_num,neg_num))\n",
    "        day_total_num=len(tmp[tmp['date_']==day])\n",
    "        day_neg_num=len(tmp[(tmp['date_']==day) & (tmp[action]==0)])\n",
    "        day_pos_num=day_total_num-day_neg_num\n",
    "        print('day pos num:{};neg num:{},day total num:{}'.format(day_pos_num,day_neg_num,day_total_num))\n",
    "        df_arr.append(tmp)\n",
    "    return df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把采样的history数据 拼接上 u i特征\n",
    "def sample_concat(sample_arr,day):\n",
    "    # 用户基本特征\n",
    "    df_users=pd.read_csv(DATASET_PATH+'/user_info.csv')\n",
    "    df_users = df_users.set_index('userid')\n",
    "    # 用户统计特征\n",
    "    df_users_static=pd.read_csv(DATASET_PATH+'/user_feature_sum_avg.csv')\n",
    "    df_users_static=df_users_static.drop_duplicates(subset=['userid','date_'], keep='last')\n",
    "    df_users_static=df_users_static.set_index(['userid','date_']) # 必须重新设置idx 不然join的时候报错\n",
    "    # 视频特征\n",
    "    df_feed=pd.read_csv(DATASET_PATH+'/feed_feature.csv')\n",
    "    df_feed = df_feed.set_index('feedid')\n",
    "    \n",
    "    for index, sample in enumerate(sample_arr):\n",
    "        features = [\"userid\", \"feedid\", \"device\", \"authorid\", \"bgm_song_id\", \"bgm_singer_id\",\\\n",
    "                    'watch_count_group','video_time_group','feed_cluter',\\\n",
    "                    \"videoplayseconds\",\"watch_count\",\"play_times\",'date_','des_words','ocr_words','asr_words',\\\n",
    "                    'manual_tag','machine_tag','manual_keywords','machine_keywords','feed_emb_id']\n",
    "        features=features+['user_'+b+'_sum_group' for b in FEA_COLUMN_LIST]+['user_'+b+'_mean_group' for b in FEA_COLUMN_LIST]\n",
    "        \n",
    "        action=ACTION_LIST[index]\n",
    "        print(action)\n",
    "        sample = sample.join(df_feed, on=\"feedid\", how=\"left\", rsuffix=\"_feed\")\n",
    "        sample = sample.join(df_users, on=[\"userid\"], how=\"left\", rsuffix=\"_user_id\")\n",
    "        sample = sample.join(df_users_static, on=[\"userid\", \"date_\"], how=\"left\", rsuffix=\"_user_static\")\n",
    "        \n",
    "        # 把各种统计信息更新到features中\n",
    "        user_feature_col = [b+\"_sum\" for b in FEA_COLUMN_LIST]+[b+\"_mean\" for b in FEA_COLUMN_LIST]\n",
    "        sample[user_feature_col] = sample[user_feature_col].fillna(0.0)\n",
    "        \n",
    "        features += user_feature_col\n",
    "#         features += ACTION_LIST # 因为调整过ACTION_LIST 所以保存的sample里面的df的列又可能没有 其他的action\n",
    "        features+=[action]\n",
    "#         features+=[\"read_comment\", \"like\", \"click_avatar\",  \"forward\"]\n",
    "    \n",
    "        # id=0 填充未知分类数据和离散数据\n",
    "        sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']] += 1  \n",
    "        sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\",'watch_count_group','video_time_group']] = \\\n",
    "            sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\",\\\n",
    "                    'watch_count_group','video_time_group']].fillna(0)\n",
    "        \n",
    "        # 连续型数据进行规范化尺度\n",
    "#         sample[\"videoplayseconds\"] = np.log(sample[\"videoplayseconds\"] + 1.0)\n",
    "#         sample[\"watch_count\"] = np.log(sample[\"watch_count\"] + 1.0)\n",
    "        \n",
    "        # 给数值型数据增加非线性\n",
    "        dense_cols=['videoplayseconds','watch_count']+user_feature_col\n",
    "#         for c in dense_cols:\n",
    "#             # 先log再增加非线性\n",
    "#             sample[c] = np.log(sample[c] + 1.0)\n",
    "#             # log\n",
    "#             sample['{}_log'.format(c)]=np.log(sample[c] + 1.0)\n",
    "#             # ^2\n",
    "#             sample['{}_square'.format(c)]=np.square(sample[c])\n",
    "#             # e\n",
    "#             sample['{}_exp'.format(c)]=np.exp(sample[c])\n",
    "            \n",
    "#         features += [b+'_log' for b in dense_cols]\n",
    "#         features += [b+'_square' for b in dense_cols]\n",
    "#         features += [b+'_exp' for b in dense_cols]\n",
    "        # 把分类数据id转化成int格式\n",
    "        sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']] = \\\n",
    "            sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']].astype(int)\n",
    "        \n",
    "        file_path='{}/generater_data/{}_{}_concat_sample.csv'.format(ROOT_PATH,action,day)\n",
    "        print('Save to: {}'.format(file_path))\n",
    "        sample[features].to_csv(file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO在concat时 进行数值特征非线性转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把test数据 拼接上 u i特征\n",
    "def test_concat(df_test):\n",
    "    # 用户基本特征\n",
    "    df_users=pd.read_csv(DATASET_PATH+'/user_info.csv')\n",
    "    df_users = df_users.set_index('userid')\n",
    "    # 用户统计特征\n",
    "    df_users_static=pd.read_csv(DATASET_PATH+'/user_feature_sum_avg.csv')\n",
    "    df_users_static=df_users_static.drop_duplicates(subset=['userid','date_'], keep='last')\n",
    "    # test的时候直接使用14天的统计数据\n",
    "    df_users_static=df_users_static[df_users_static['date_']==14]\n",
    "    df_users_static=df_users_static.set_index('userid')\n",
    "    \n",
    "    # 视频特征\n",
    "    df_feed=pd.read_csv(DATASET_PATH+'/feed_feature.csv')\n",
    "    df_feed = df_feed.set_index('feedid')\n",
    "    \n",
    "    features = [\"userid\", \"feedid\", \"device\", \"authorid\", \"bgm_song_id\", \"bgm_singer_id\",\\\n",
    "                'watch_count_group','video_time_group','feed_cluter',\\\n",
    "                \"videoplayseconds\",\"watch_count\",\"play_times\",'des_words','ocr_words','asr_words',\\\n",
    "                'manual_tag','machine_tag','manual_keywords','machine_keywords','feed_emb_id']\n",
    "    \n",
    "    features=features+['user_'+b+'_sum_group' for b in FEA_COLUMN_LIST]+['user_'+b+'_mean_group' for b in FEA_COLUMN_LIST]\n",
    "\n",
    "    sample=df_test\n",
    "    sample = sample.join(df_feed, on=\"feedid\", how=\"left\", rsuffix=\"_feed\")\n",
    "    sample = sample.join(df_users, on=\"userid\", how=\"left\", rsuffix=\"_user_id\")\n",
    "    sample = sample.join(df_users_static, on=\"userid\", how=\"left\", rsuffix=\"_user_static\")\n",
    "\n",
    "    # 把各种统计信息更新到features中\n",
    "    user_feature_col = [b+\"_sum\" for b in FEA_COLUMN_LIST]+[b+\"_mean\" for b in FEA_COLUMN_LIST]\n",
    "    # test中可能有冷启动 所以必须填充空值\n",
    "    sample[user_feature_col] = sample[user_feature_col].fillna(0.0)\n",
    "\n",
    "    features += user_feature_col\n",
    "#     features += ACTION_LIST #test里没有ACTION_LIST了\n",
    "\n",
    "    # id=0 填充未知分类数据和离散数据\n",
    "    sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']] += 1  \n",
    "    sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\",'watch_count_group','video_time_group']] = \\\n",
    "        sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\",\\\n",
    "                'watch_count_group','video_time_group']].fillna(0)\n",
    "\n",
    "    # 连续型数据进行规范化尺度\n",
    "#     sample[\"videoplayseconds\"] = np.log(sample[\"videoplayseconds\"] + 1.0)\n",
    "# #         sample[\"watch_count\"] = np.log(sample[\"watch_count\"] + 1.0)\n",
    "\n",
    "    dense_cols=['videoplayseconds','watch_count']+user_feature_col\n",
    "#     for c in dense_cols:\n",
    "#         # 先log再增加非线性\n",
    "#         sample[c] = np.log(sample[c] + 1.0)\n",
    "#         # log\n",
    "#         sample['{}_log'.format(c)]=np.log(sample[c] + 1.0)\n",
    "#         # ^2\n",
    "#         sample['{}_square'.format(c)]=np.square(sample[c])\n",
    "#         # e\n",
    "#         sample['{}_exp'.format(c)]=np.exp(sample[c])\n",
    "\n",
    "#     features += [b+'_log' for b in dense_cols]\n",
    "#     features += [b+'_square' for b in dense_cols]\n",
    "#     features += [b+'_exp' for b in dense_cols]\n",
    "\n",
    "    # 把分类数据id转化成int格式\n",
    "    sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']] = \\\n",
    "        sample[[\"authorid\", \"bgm_song_id\", \"bgm_singer_id\",'watch_count_group','video_time_group']].astype(int)\n",
    "\n",
    "    file_path='{}/test_a_concat.csv'.format(DATASET_PATH)\n",
    "    print('Save to: {}'.format(file_path))\n",
    "    sample[features].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------read_comment-------------\n",
      "pos num:249710;neg num:3745650\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/read_comment_14_sample.csv\n",
      "(3995360, 8)\n",
      "-----------like-------------\n",
      "pos num:183556;neg num:2753340\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/like_14_sample.csv\n",
      "(2936896, 8)\n",
      "-----------click_avatar-------------\n",
      "pos num:52876;neg num:528760\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/click_avatar_14_sample.csv\n",
      "(581636, 8)\n",
      "-----------forward-------------\n",
      "pos num:27161;neg num:271610\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/forward_14_sample.csv\n",
      "(298771, 8)\n",
      "generate sample history data cost time 24.78s\n"
     ]
    }
   ],
   "source": [
    "# 1 进行history采样\n",
    "t0=time.time()\n",
    "df_arr=generate_sample(14)\n",
    "t1=time.time()\n",
    "print('generate sample history data cost time {:.2f}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------read_comment---------\n",
      "(3995360, 8)\n",
      "pos num:249710;neg num:3745650\n",
      "day pos num:20597;neg num:317305,day total num:337902\n",
      "--------like---------\n",
      "(2936896, 8)\n",
      "pos num:183556;neg num:2753340\n",
      "day pos num:14614;neg num:233385,day total num:247999\n",
      "--------click_avatar---------\n",
      "(581636, 8)\n",
      "pos num:52876;neg num:528760\n",
      "day pos num:4540;neg num:44722,day total num:49262\n",
      "--------forward---------\n",
      "(298771, 8)\n",
      "pos num:27161;neg num:271610\n",
      "day pos num:2061;neg num:23138,day total num:25199\n"
     ]
    }
   ],
   "source": [
    "# 1 或者通过读取已有文件 获取采样数据\n",
    "df_arr=get_generate_sample(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_comment\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/read_comment_14_concat_sample.csv\n",
      "like\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/like_14_concat_sample.csv\n",
      "click_avatar\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/click_avatar_14_concat_sample.csv\n",
      "forward\n",
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/generater_data/forward_14_concat_sample.csv\n",
      "concat sample history and user/feed feature cost time 730.38s\n"
     ]
    }
   ],
   "source": [
    "# 2 采样之后进行concat\n",
    "t0=time.time()\n",
    "sample_concat(df_arr,14)\n",
    "t1=time.time()\n",
    "print('concat sample history and user/feed feature cost time {:.2f}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to: /testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data/wechat_algo_data1/test_a_concat.csv\n",
      "concat test history and user/feed feature cost time 34.44s\n"
     ]
    }
   ],
   "source": [
    "# 3 对于test数据也要进行concat (因为添加了附加信息)\n",
    "df_test=pd.read_csv(TEST_FILE)\n",
    "t0=time.time()\n",
    "test_concat(df_test)\n",
    "t1=time.time()\n",
    "print('concat test history and user/feed feature cost time {:.2f}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO 对于train test的所有id类型 需不需要Label_encoder\n",
    "#### TODO 如果Label_encoder 再这里进行 还是train_model之前进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
