{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 原始数据路径\n",
    "ROOT_PATH = \"/testcbd017_gujinfang/GJFCode/WeChat_2021/Code/data\"\n",
    "DATA_PATH=ROOT_PATH+'/wechat_algo_data1'\n",
    "SAVE_PATH=DATA_PATH+'/tree_feature'\n",
    "SUB_PATH=ROOT_PATH+'/submit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = ['read_comment', 'like', 'click_avatar', 'forward', 'favorite', 'comment', 'follow']\n",
    "# y_list = ['read_comment', 'like', 'click_avatar', 'forward']\n",
    "max_day = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从官方baseline里面抽出来的评测函数\n",
    "def uAUC(labels, preds, user_id_list):\n",
    "    \"\"\"Calculate user AUC\"\"\"\n",
    "    user_pred = defaultdict(lambda: [])\n",
    "    user_truth = defaultdict(lambda: [])\n",
    "\n",
    "    for idx, truth in enumerate(labels):\n",
    "        user_id = user_id_list[idx]\n",
    "        pred = preds[idx]\n",
    "        truth = labels[idx]\n",
    "        user_pred[user_id].append(pred)\n",
    "        user_truth[user_id].append(truth)\n",
    "\n",
    "    user_flag = defaultdict(lambda: False)\n",
    "\n",
    "    for user_id in set(user_id_list):\n",
    "        truths = user_truth[user_id]\n",
    "        flag = False\n",
    "        # 若全是正样本或全是负样本，则flag为False\n",
    "        for i in range(len(truths) - 1):\n",
    "            if truths[i] != truths[i + 1]:\n",
    "                flag = True\n",
    "                break\n",
    "\n",
    "        user_flag[user_id] = flag\n",
    "\n",
    "    total_auc = 0.0\n",
    "    size = 0.0\n",
    "\n",
    "    for user_id in user_flag:\n",
    "        if user_flag[user_id]:\n",
    "            auc = roc_auc_score(np.asarray(user_truth[user_id]), np.asarray(user_pred[user_id]))\n",
    "            total_auc += auc \n",
    "            size += 1.0\n",
    "\n",
    "    user_auc = float(total_auc)/size\n",
    "    return user_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把tag keyword弄成multi-hot\n",
    "def process_multi_hot(df,col,prefix):\n",
    "    print('缺失比例:{}'.format(df[col].isna().mean()))\n",
    "    print(df.loc[0,col])\n",
    "    multi_hot_col=df[col].str.replace(';','|').str.get_dummies()\n",
    "    multi_hot_col.columns=[prefix+'_'+na for na in list(multi_hot_col.columns)]\n",
    "    return multi_hot_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_lillte_col(df,rate=0.01):\n",
    "    drop_cols=[]\n",
    "    for c in df.columns:\n",
    "        if(df[c].mean()*100<rate):\n",
    "            drop_cols.append(c)\n",
    "    n1,n2=len(df.columns),len(drop_cols)\n",
    "    print('all cols:{},drop cols:{}'.format(n1,n2))\n",
    "    return df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_info = pd.read_csv(DATA_PATH+'/feed_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feedid', 'authorid', 'videoplayseconds', 'description', 'ocr', 'asr',\n",
      "       'bgm_song_id', 'bgm_singer_id', 'manual_keyword_list',\n",
      "       'machine_keyword_list', 'manual_tag_list', 'machine_tag_list',\n",
      "       'description_char', 'ocr_char', 'asr_char', 'video_time_group',\n",
      "       'feed_cluter', 'des_words', 'ocr_words', 'asr_words', 'manual_tag',\n",
      "       'machine_tag', 'manual_keywords', 'machine_keywords', 'feed_emb_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(feed_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cols:350,drop cols:110\n"
     ]
    }
   ],
   "source": [
    "manual_tag_multi_col_drop=drop_lillte_col(manual_tag_multi_col,rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_info=feed_info.join(manual_tag_multi_col_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feedid', 'authorid', 'videoplayseconds', 'description', 'ocr', 'asr',\n",
      "       'bgm_song_id', 'bgm_singer_id', 'manual_keyword_list',\n",
      "       'machine_keyword_list',\n",
      "       ...\n",
      "       'tag_9', 'tag_91', 'tag_92', 'tag_93', 'tag_94', 'tag_95', 'tag_96',\n",
      "       'tag_97', 'tag_98', 'tag_99'],\n",
      "      dtype='object', length=265)\n"
     ]
    }
   ],
   "source": [
    "print(feed_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失比例:0.009544925031002217\n",
      "81;269;159;6\n",
      "all cols:350,drop cols:110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [08:02<03:55, 235.85s/it]"
     ]
    }
   ],
   "source": [
    "## 读取训练集\n",
    "train = pd.read_csv(DATA_PATH+'/user_action.csv')\n",
    "\n",
    "## 读取测试集\n",
    "test = pd.read_csv(DATA_PATH+'/test_a.csv')\n",
    "test['date_'] = max_day\n",
    "\n",
    "# 合并处理\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "## 读取视频特征表\n",
    "feed_info = pd.read_csv(DATA_PATH+'/feed_feature.csv')\n",
    "\n",
    "## 挑选feed feature列\n",
    "feed_info = feed_info[[\n",
    "    'feedid', 'authorid', 'videoplayseconds','bgm_song_id', 'bgm_singer_id','video_time_group','feed_cluter','manual_tag_list'\n",
    "]]\n",
    "multi_tag=process_multi_hot(feed_info,'manual_tag_list','tag')\n",
    "multi_tag=drop_lillte_col(multi_tag,rate=0.02)\n",
    "tag_cols=list(multi_tag.columns)\n",
    "feed_info=feed_info.join(multi_tag)\n",
    "\n",
    "df = df.merge(feed_info, on='feedid', how='left')\n",
    "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "df['videoplayseconds'] *= 1000\n",
    "\n",
    "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "df['is_finish'] = (df['play'] >= 0.9*df['videoplayseconds']).astype('int8')\n",
    "df['play_times'] = df['play'] / df['videoplayseconds']\n",
    "\n",
    "play_cols = [\n",
    "    'is_finish', 'play_times', 'play', 'stay'\n",
    "]\n",
    "\n",
    "# 滑窗统计\n",
    "## 统计历史5天的曝光、转化、视频观看等情况（此处的转化率统计其实就是target encoding）\n",
    "n_day = 5\n",
    "\n",
    "\n",
    "for stat_cols in tqdm([\n",
    "    ['userid'],\n",
    "    ['feedid'],\n",
    "    ['authorid'],\n",
    "#     ['video_time_group'],\n",
    "#     ['feed_cluter'],\n",
    "#     ['userid', 'authorid'],\n",
    "#     ['userid', 'feed_cluter'],\n",
    "#     ['userid', 'video_time_group'],\n",
    "]):\n",
    "\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "        left, right = max(target_day - n_day, 1), target_day - 1\n",
    "        tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
    "        tmp['date_'] = target_day\n",
    "\n",
    "        # 使用transform使得输出和输入 数量上对齐 直接用.sum() 其输出相当于set 数量上对不齐\n",
    "        tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
    "        g = tmp.groupby(stat_cols)\n",
    "        tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
    "        feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
    "\n",
    "        # 统计近期当前特征列各类 play_times, play, stay\n",
    "        for x in play_cols[1:]:\n",
    "            for stat in ['max','mean','median']:\n",
    "                tmp['{}_{}day_{}_{}'.format(f, n_day, x, stat)] = g[x].transform(stat)\n",
    "                feats.append('{}_{}day_{}_{}'.format(f, n_day, x, stat))\n",
    "        # 统计各类 近期对于target的比例\n",
    "        for y in y_list[:4]:\n",
    "            tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
    "            tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
    "            feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
    "\n",
    "        tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
    "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del g, tmp\n",
    "\n",
    "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "    del stat_df\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "## 全局信息统计，包括曝光、偏好等，略有穿越，但问题不大，可以上分，只要注意不要对userid-feedid做组合统计就行\n",
    "all_static_cols=['userid', 'feedid', 'authorid','feed_cluter','video_time_group','bgm_song_id','bgm_singer_id']+tag_cols\n",
    "for f in tqdm(all_static_cols):\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    \n",
    "## tag统计\n",
    "\n",
    "\n",
    "for f1, f2 in tqdm([\n",
    "    ['userid', 'feedid'],\n",
    "    ['userid', 'authorid']\n",
    "]):\n",
    "\n",
    "    df['{}_in_{}_nunique'.format(f1, f2)] = df.groupby(f2)[f1].transform('nunique')\n",
    "    df['{}_in_{}_nunique'.format(f2, f1)] = df.groupby(f1)[f2].transform('nunique')\n",
    "\n",
    "for f1, f2 in tqdm([\n",
    "    ['userid', 'authorid'],\n",
    "    ['userid', 'feed_cluter'],\n",
    "]):\n",
    "\n",
    "    df['{}_{}_count'.format(f1, f2)] = df.groupby([f1, f2])['date_'].transform('count')\n",
    "    df['{}_in_{}_count_prop'.format(f1, f2)] = df['{}_{}_count'.format(f1, f2)] / (df[f2 + '_count'] + 1)\n",
    "    df['{}_in_{}_count_prop'.format(f2, f1)] = df['{}_{}_count'.format(f1, f2)] / (df[f1 + '_count'] + 1)\n",
    "\n",
    "df['videoplayseconds_in_userid_mean'] = df.groupby('userid')['videoplayseconds'].transform('mean')\n",
    "df['videoplayseconds_in_authorid_mean'] = df.groupby('authorid')['videoplayseconds'].transform('mean')\n",
    "df['videoplayseconds_in_feed_cluter'] = df.groupby('feed_cluter')['videoplayseconds'].transform('mean')\n",
    "df['feedid_in_authorid_nunique'] = df.groupby('authorid')['feedid'].transform('nunique')\n",
    "\n",
    "## 内存够用的不需要做这一步\n",
    "#df = reduce_mem(df, [f for f in df.columns if f not in ['date_'] + play_cols + y_list])\n",
    "\n",
    "train = df[~df['read_comment'].isna()].reset_index(drop=True)\n",
    "test = df[df['read_comment'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基于原始数据进行特征工程、制作出全特征的train test 保存\n",
    "def data_feature_make():\n",
    "    ## 读取训练集\n",
    "    train = pd.read_csv(DATA_PATH+'/user_action.csv')\n",
    "    \n",
    "    ## 读取测试集\n",
    "    test = pd.read_csv(DATA_PATH+'/test_a.csv')\n",
    "    test['date_'] = max_day\n",
    "\n",
    "    # 合并处理\n",
    "    df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    ## 读取视频特征表\n",
    "    feed_info = pd.read_csv(DATA_PATH+'/feed_feature.csv')\n",
    "\n",
    "    ## 挑选feed feature列\n",
    "    feed_info = feed_info[[\n",
    "        'feedid', 'authorid', 'videoplayseconds','bgm_song_id', 'bgm_singer_id','video_time_group','feed_cluter'\n",
    "    ]]\n",
    "\n",
    "    df = df.merge(feed_info, on='feedid', how='left')\n",
    "    ## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "    df['videoplayseconds'] *= 1000\n",
    "\n",
    "    ## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "    df['is_finish'] = (df['play'] >= 0.9*df['videoplayseconds']).astype('int8')\n",
    "    df['play_times'] = df['play'] / df['videoplayseconds']\n",
    "\n",
    "    play_cols = [\n",
    "        'is_finish', 'play_times', 'play', 'stay'\n",
    "    ]\n",
    "\n",
    "    # 滑窗统计\n",
    "    ## 统计历史5天的曝光、转化、视频观看等情况（此处的转化率统计其实就是target encoding）\n",
    "    n_day = 5\n",
    "    \n",
    "\n",
    "    for stat_cols in tqdm([\n",
    "        ['userid'],\n",
    "        ['feedid'],\n",
    "        ['authorid'],\n",
    "#         ['bgm_song_id'],\n",
    "#         ['bgm_singer_id'],\n",
    "#         ['video_time_group'],\n",
    "#         ['feed_cluter'],\n",
    "#         ['userid', 'authorid'],\n",
    "#         ['userid', 'feed_cluter'],\n",
    "#         ['userid', 'video_time_group'],\n",
    "    ]):\n",
    "\n",
    "        f = '_'.join(stat_cols)\n",
    "        stat_df = pd.DataFrame()\n",
    "        for target_day in range(2, max_day + 1):\n",
    "            left, right = max(target_day - n_day, 1), target_day - 1\n",
    "            tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
    "            tmp['date_'] = target_day\n",
    "            \n",
    "            # 使用transform使得输出和输入 数量上对齐 直接用.sum() 其输出相当于set 数量上对不齐\n",
    "            tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
    "            g = tmp.groupby(stat_cols)\n",
    "            tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
    "            feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
    "\n",
    "            # 统计近期当前特征列各类 play_times, play, stay\n",
    "            for x in play_cols[1:]:\n",
    "                for stat in ['min','max', 'mean','median']:\n",
    "                    tmp['{}_{}day_{}_{}'.format(f, n_day, x, stat)] = g[x].transform(stat)\n",
    "                    feats.append('{}_{}day_{}_{}'.format(f, n_day, x, stat))\n",
    "            # 统计各类 近期对于target的比例\n",
    "            for y in y_list[:4]:\n",
    "                tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
    "                tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
    "                feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
    "\n",
    "            tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
    "            stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "            del g, tmp\n",
    "\n",
    "        df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "        del stat_df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    ## 全局信息统计，包括曝光、偏好等，略有穿越，但问题不大，可以上分，只要注意不要对userid-feedid做组合统计就行\n",
    "    for f in tqdm(['userid', 'feedid', 'authorid','feed_cluter','video_time_group','bgm_song_id','bgm_singer_id']):\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "\n",
    "    for f1, f2 in tqdm([\n",
    "        ['userid', 'feedid'],\n",
    "        ['userid', 'authorid']\n",
    "    ]):\n",
    "\n",
    "        df['{}_in_{}_nunique'.format(f1, f2)] = df.groupby(f2)[f1].transform('nunique')\n",
    "        df['{}_in_{}_nunique'.format(f2, f1)] = df.groupby(f1)[f2].transform('nunique')\n",
    "\n",
    "    for f1, f2 in tqdm([\n",
    "        ['userid', 'authorid'],\n",
    "        ['userid', 'feed_cluter'],\n",
    "        ['userid', 'video_time_group'],\n",
    "    ]):\n",
    "\n",
    "        df['{}_{}_count'.format(f1, f2)] = df.groupby([f1, f2])['date_'].transform('count')\n",
    "        df['{}_in_{}_count_prop'.format(f1, f2)] = df['{}_{}_count'.format(f1, f2)] / (df[f2 + '_count'] + 1)\n",
    "        df['{}_in_{}_count_prop'.format(f2, f1)] = df['{}_{}_count'.format(f1, f2)] / (df[f1 + '_count'] + 1)\n",
    "\n",
    "    df['videoplayseconds_in_userid_mean'] = df.groupby('userid')['videoplayseconds'].transform('mean')\n",
    "    df['videoplayseconds_in_authorid_mean'] = df.groupby('authorid')['videoplayseconds'].transform('mean')\n",
    "    df['videoplayseconds_in_feed_cluter'] = df.groupby('feed_cluter')['videoplayseconds'].transform('mean')\n",
    "    df['feedid_in_authorid_nunique'] = df.groupby('authorid')['feedid'].transform('nunique')\n",
    "\n",
    "    ## 内存够用的不需要做这一步\n",
    "    #df = reduce_mem(df, [f for f in df.columns if f not in ['date_'] + play_cols + y_list])\n",
    "\n",
    "    train = df[~df['read_comment'].isna()].reset_index(drop=True)\n",
    "    test = df[df['read_comment'].isna()].reset_index(drop=True)\n",
    "    \n",
    "#     train.to_csv(DATA_PATH+'/tree_train.csv',index=False)\n",
    "#     test.to_csv(DATA_PATH+'/tree_test.csv',index=False)\n",
    "#     print('save ok!')\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据path读取全量trian 然后分出真正的train val\n",
    "def make_train_val(df,day=14):\n",
    "    # 写法1：前t-1天预测t天\n",
    "    train=df[(df['date_']<day) & (df['date_']>=day-7)].reset_index(drop=True)\n",
    "    day=min(day,14)\n",
    "    val=df[df['date_']==day].reset_index(drop=True)\n",
    "    return train,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集分数\n",
    "def train_val(train,val,test):\n",
    "    play_cols = [\n",
    "        'is_finish', 'play_times', 'play', 'stay'\n",
    "    ]\n",
    "    cols = [f for f in train.columns if f not in ['date_'] + play_cols + y_list]\n",
    "    uauc_list = []\n",
    "    r_list = []\n",
    "    \n",
    "    for y in y_list[:4]:\n",
    "        print('=========', y, '=========')\n",
    "        t = time.time()\n",
    "        clf = LGBMClassifier(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=10,\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=2021,\n",
    "            metric='None'\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            train[cols], train[y],\n",
    "            eval_set=[(val[cols], val[y])],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=5,\n",
    "            verbose=5\n",
    "        )\n",
    "\n",
    "        val[y + '_score'] = clf.predict_proba(val[cols])[:, 1]\n",
    "        val_uauc = uAUC(val[y], val[y + '_score'], val['userid'])\n",
    "        uauc_list.append(val_uauc)\n",
    "        print(val_uauc)\n",
    "\n",
    "        r_list.append(clf.best_iteration_)\n",
    "        print('runtime: {}\\n'.format(time.time() - t))\n",
    "\n",
    "\n",
    "    weighted_uauc = 0.4 * uauc_list[0] + 0.3 * uauc_list[1] + 0.2 * uauc_list[2] + 0.1 * uauc_list[3]\n",
    "    print(uauc_list)\n",
    "    print(weighted_uauc)\n",
    "    \n",
    "    ##################### 全量训练 #####################\n",
    "\n",
    "    train=pd.concat([train, val], axis=0, ignore_index=True)\n",
    "    r_dict = dict(zip(y_list[:4], r_list))\n",
    "    for y in y_list[:4]:\n",
    "        print('=========', y, '=========')\n",
    "        t = time.time()\n",
    "        clf = LGBMClassifier(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=r_dict[y],\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=2021\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            train[cols], train[y],\n",
    "            eval_set=[(train[cols], train[y])],\n",
    "            early_stopping_rounds=r_dict[y],\n",
    "            verbose=100\n",
    "        )\n",
    "\n",
    "        test[y] = clf.predict_proba(test[cols])[:, 1]\n",
    "        print('runtime: {}\\n'.format(time.time() - t))\n",
    "\n",
    "    test[['userid', 'feedid'] + y_list[:4]].to_csv(\n",
    "        SUB_PATH+'/tree_%.6f_%.6f_%.6f_%.6f_%.6f.csv' % (weighted_uauc, uauc_list[0], uauc_list[1], uauc_list[2], uauc_list[3]),\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:56<00:00, 38.90s/it]\n",
      "100%|██████████| 7/7 [00:01<00:00,  3.78it/s]\n",
      "100%|██████████| 2/2 [00:13<00:00,  6.99s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make concat and save all feature cost time 167.43s\n"
     ]
    }
   ],
   "source": [
    "# 1 制作特征保存全量数据\n",
    "t0=time.time()\n",
    "train,test=data_feature_make()\n",
    "t1=time.time()\n",
    "print('make concat and save all feature cost time {:.2f}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 读取train test 将train 划分成真正的train_val\n",
    "# test=pd.read_csv(DATA_PATH+'/tree_test.csv')\n",
    "# df=pd.read_csv(DATA_PATH+'/tree_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1,val=make_train_val(train,day=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= read_comment =========\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\tvalid_0's auc: 0.918807\n",
      "[10]\tvalid_0's auc: 0.923457\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's auc: 0.923457\n",
      "0.5844397744692892\n",
      "runtime: 47.613232135772705\n",
      "\n",
      "========= like =========\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\tvalid_0's auc: 0.822387\n",
      "[10]\tvalid_0's auc: 0.82679\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's auc: 0.82679\n",
      "0.5818755701885522\n",
      "runtime: 39.360753297805786\n",
      "\n",
      "========= click_avatar =========\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\tvalid_0's auc: 0.846806\n",
      "[10]\tvalid_0's auc: 0.853525\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's auc: 0.853525\n",
      "0.7245278426081294\n",
      "runtime: 38.6142635345459\n",
      "\n",
      "========= forward =========\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\tvalid_0's auc: 0.848859\n",
      "[10]\tvalid_0's auc: 0.854375\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's auc: 0.854375\n",
      "0.6727491565152935\n",
      "runtime: 48.620487451553345\n",
      "\n",
      "[0.5844397744692892, 0.5818755701885522, 0.7245278426081294, 0.6727491565152935]\n",
      "0.6205190650174366\n",
      "========= read_comment =========\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.114912\n",
      "runtime: 72.37620329856873\n",
      "\n",
      "========= like =========\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.10022\n",
      "runtime: 55.90383815765381\n",
      "\n",
      "========= click_avatar =========\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.0362801\n",
      "runtime: 70.49012184143066\n",
      "\n",
      "========= forward =========\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.0208919\n",
      "runtime: 81.30192351341248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 线下验证\n",
    "train_val(train1,val,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-63eed297ee78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_feat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# lgb_feat_imp = pd.Series(clf.feature_importances_, cols).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
